{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d4591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d218dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 16.4% 273.5/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "w_pretrained = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efed250",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_pretrained = gensim.models.KeyedVectors.load(\"word2vec-google-news-300.kv\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f85fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laptop', 0.4873006343841553),\n",
       " ('slingshot_dragster', 0.47504204511642456),\n",
       " ('LH_Yeah', 0.4735804498195648),\n",
       " ('KNYAF', 0.466823935508728),\n",
       " ('bike', 0.4662119746208191),\n",
       " ('Fabio_complimented', 0.46549883484840393),\n",
       " ('cars', 0.4605005085468292),\n",
       " ('Audi_S4_Cabriolet', 0.45188385248184204),\n",
       " ('GREG_BIFFLE_Yeah', 0.45104363560676575),\n",
       " ('Dan_Schieler_Senior', 0.45039424300193787)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_pretrained.most_similar(positive=[\"computer\", \"university\", \"beautiful\", \"fast\", \"car\"],negative=['professor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16efa9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"computer\", \"university\", \"beautiful\", \"fast\", \"car\"]\n",
    "similar_words = {word: w_pretrained.most_similar(word, topn=5) for word in words}\n",
    "\n",
    "# Perform analogy tests\n",
    "analogy_tests = [\n",
    "    (\"king\", \"man\", \"woman\"),  # Expected to be close to 'queen'\n",
    "    (\"paris\", \"france\", \"germany\"),  # Expected to be close to 'berlin'\n",
    "    (\"apple\", \"fruit\", \"vegetable\"),  # Expected to be close to 'carrot' or similar\n",
    "]\n",
    "\n",
    "analogy_results = {f\"{w1} - {w2} + {w3}\": w_pretrained.most_similar(positive=[w1, w3], negative=[w2], topn=1) for w1, w2, w3 in analogy_tests}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fec9c96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'king - man + woman': [('queen', 0.7118193507194519)],\n",
       " 'paris - france + germany': [('berlin', 0.48413652181625366)],\n",
       " 'apple - fruit + vegetable': [('potato', 0.5865277647972107)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aab5b7",
   "metadata": {},
   "source": [
    "The below shows that you should not be reliant on pretrained data \n",
    "\n",
    "It may be that the context has not been fit properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e86e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"computer\", \"university\", \"beautiful\", \"fast\", \"car\"]\n",
    "similar_words = {word: w_pretrained.most_similar(word, topn=5) for word in words}\n",
    "\n",
    "# Perform analogy tests\n",
    "analogy_tests = [\n",
    "    (\"king\", \"man\", \"woman\"),  # Expected to be close to 'queen'\n",
    "    (\"paris\", \"france\", \"india\"),  # Expected to be close to 'berlin'\n",
    "    (\"apple\", \"fruit\", \"vegetable\"),  # Expected to be close to 'carrot' or similar\n",
    "]\n",
    "\n",
    "analogy_results = {f\"{w1} - {w2} + {w3}\": w_pretrained.most_similar(positive=[w1, w3], negative=[w2], topn=1) for w1, w2, w3 in analogy_tests}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3766447f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'king - man + woman': [('queen', 0.7118193507194519)],\n",
       " 'paris - france + india': [('chennai', 0.5442505478858948)],\n",
       " 'apple - fruit + vegetable': [('potato', 0.5865277647972107)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d38510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:\\\\SNLP\\\\IMDB_Dataset.csv\")\n",
    "\n",
    "# Basic EDA\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bacf475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  [one, reviewers, mentioned, watching, oz, epis...  \n",
      "1  [wonderful, little, production, filming, techn...  \n",
      "2  [thought, wonderful, way, spend, time, hot, su...  \n",
      "3  [basically, family, little, boy, jake, thinks,...  \n",
      "4  [petter, mattei, love, time, money, visually, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff208692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize cleaned reviews\n",
    "tokenized_reviews = df['cleaned_review'].tolist()\n",
    "\n",
    "# Train a Skip-gram model\n",
    "skipgram_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, sg=1, min_count=2)\n",
    "\n",
    "# Train a CBoW model\n",
    "cbow_model = Word2Vec(sentences=tokenized_reviews, vector_size=100, window=5, sg=0, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8909effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_vector(model, tokens):\n",
    "    # Get vectors for tokens that are in the model's vocabulary\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "df['skipgram_vectors'] = df['cleaned_review'].apply(lambda x: get_review_vector(skipgram_model, x))\n",
    "df['cbow_vectors'] = df['cleaned_review'].apply(lambda x: get_review_vector(cbow_model, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d735697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Model  Accuracy  F1 Score\n",
      "0  Skip-gram    0.8768  0.878044\n",
      "1       CBoW    0.8644  0.866509\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Prepare the data\n",
    "X_skipgram = np.array(df['skipgram_vectors'].tolist())\n",
    "X_cbow = np.array(df['cbow_vectors'].tolist())\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "X_train_sg, X_test_sg, y_train, y_test = train_test_split(X_skipgram, y, test_size=0.2, random_state=42)\n",
    "X_train_cb, X_test_cb, _, _ = train_test_split(X_cbow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate models\n",
    "models = {\n",
    "    'Skip-gram': (X_train_sg, X_test_sg),\n",
    "    'CBoW': (X_train_cb, X_test_cb)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, (X_train, X_test) in models.items():\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results.append((model_name, accuracy, f1))\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'F1 Score'])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a92fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Model  Accuracy  F1 Score\n",
      "0  Skip-gram    0.8768  0.878044\n",
      "1       CBoW    0.8644  0.866509\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dca609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5aae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602111e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f481ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64424b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
