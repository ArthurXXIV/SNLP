{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d3b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f380122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\SNLP\\IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49869f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "444b0086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46bc6427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:14<00:00, 667.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "3  Basically there's a family where a little boy ...   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
      "\n",
      "                                        clean_review  \n",
      "0  one reviewers mentioned watching oz episode ho...  \n",
      "1  wonderful little production filming technique ...  \n",
      "2  thought wonderful way spend time hot summer we...  \n",
      "3  basically family little boy jake thinks zombie...  \n",
      "4  petter mattei love time money visually stunnin...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(\"C:/SNLP/IMDB_Dataset.csv\")\n",
    "print(df.head())\n",
    "# Data Cleaning\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "tqdm.pandas()\n",
    "df['clean_review'] = df['review'].progress_apply(clean_text)\n",
    "print(df[['review', 'clean_review']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff08b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Padding\n",
    "from collections import Counter\n",
    "\n",
    "# Build vocabulary\n",
    "all_text = ' '.join(df['clean_review'])\n",
    "words = all_text.split()\n",
    "count_words = Counter(words)\n",
    "total_words = len(words)\n",
    "vocab = sorted(count_words, key=count_words.get, reverse=True)\n",
    "vocab_to_int = {word: idx+1 for idx, word in enumerate(vocab)}  # idx+1 because 0 is used for padding\n",
    "\n",
    "def encode_text(text):\n",
    "    tokens = text.split()\n",
    "    encoded = [vocab_to_int.get(word, 0) for word in tokens]  # Use 0 if word not found\n",
    "    return encoded\n",
    "\n",
    "df['encoded_review'] = df['clean_review'].apply(encode_text)\n",
    "\n",
    "# Determine max sequence length\n",
    "max_len = 200\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    if len(seq) >= max_len:\n",
    "        return seq[:max_len]\n",
    "    else:\n",
    "        return [0]*(max_len - len(seq)) + seq\n",
    "\n",
    "df['padded_review'] = df['encoded_review'].apply(lambda x: pad_sequence(x, max_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db7605e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "label_to_int = {'positive': 1, 'negative': 0}\n",
    "df['label'] = df['sentiment'].map(label_to_int)\n",
    "\n",
    "# Split Data\n",
    "X = np.array(df['padded_review'].tolist())\n",
    "y = np.array(df['label'].tolist())\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0bdc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Dataset\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = torch.tensor(reviews, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = IMDBDataset(X_train, y_train)\n",
    "val_dataset = IMDBDataset(X_val, y_val)\n",
    "test_dataset = IMDBDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef1eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe embeddings...\n",
      "Download completed.\n",
      "Extracting GloVe embeddings...\n",
      "Extraction completed.\n"
     ]
    }
   ],
   "source": [
    "# Download GloVe Embeddings\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_glove_embeddings():\n",
    "    glove_zip = 'glove.6B.zip'\n",
    "    if not os.path.exists(glove_zip):\n",
    "        print(\"Downloading GloVe embeddings...\")\n",
    "        url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(glove_zip, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(\"Download completed.\")\n",
    "    else:\n",
    "        print(\"GloVe embeddings already downloaded.\")\n",
    "    \n",
    "    # Extract embeddings\n",
    "    if not os.path.exists('glove.6B.100d.txt'):\n",
    "        print(\"Extracting GloVe embeddings...\")\n",
    "        with zipfile.ZipFile(glove_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "        print(\"Extraction completed.\")\n",
    "    else:\n",
    "        print(\"GloVe embeddings already extracted.\")\n",
    "\n",
    "download_glove_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9391a17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 400000/400000 [00:33<00:00, 11770.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded.\n"
     ]
    }
   ],
   "source": [
    "# Create Embedding Matrix\n",
    "embedding_dim = 100\n",
    "embedding_index = {}\n",
    "\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=400000):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "print(\"GloVe embeddings loaded.\")\n",
    "\n",
    "# Prepare embedding matrix\n",
    "vocab_size = len(vocab_to_int) + 1  # +1 for padding token\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, idx in vocab_to_int.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        # Initialize with random vector if not found\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d423b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2b66bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        val_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {np.mean(train_losses):.4f}, Validation Loss: {np.mean(val_losses):.4f}')\n",
    "        \n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc994137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9087606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, embedding_matrix, drop_prob=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer with pre-trained weights\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embeds)\n",
    "        out = rnn_out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e2c751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [14:53<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.6673, Validation Loss: 0.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [12:29<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Training Loss: 0.6747, Validation Loss: 0.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [10:49<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Training Loss: 0.6450, Validation Loss: 0.6213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [11:12<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Training Loss: 0.6147, Validation Loss: 0.5827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [13:18<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training Loss: 0.5562, Validation Loss: 0.6938\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "model_rnn_glove = RNNModel(vocab_size, embedding_dim, hidden_dim, n_layers, embedding_matrix)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_rnn_glove.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model_rnn_glove, train_loader, val_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ace4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf4adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, embedding_matrix, drop_prob=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer with pre-trained weights\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embeds)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372b55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "081af667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [13:29<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.4545, Validation Loss: 0.3230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [10:30<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Training Loss: 0.2577, Validation Loss: 0.3020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [11:39<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Training Loss: 0.1410, Validation Loss: 0.3293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [12:34<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Training Loss: 0.0665, Validation Loss: 0.4186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [12:46<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training Loss: 0.0290, Validation Loss: 0.6069\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model_lstm_glove = LSTMModel(vocab_size, embedding_dim, hidden_dim, n_layers, embedding_matrix)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_lstm_glove.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model_lstm_glove, train_loader, val_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a38055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55889106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelOnTheFly(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(RNNModelOnTheFly, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Trainable embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        rnn_out, hidden = self.rnn(embeds)\n",
    "        out = rnn_out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94149081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ef2eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [19:22<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.6943, Validation Loss: 0.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [16:58<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Training Loss: 0.6777, Validation Loss: 0.6776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [16:52<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Training Loss: 0.6410, Validation Loss: 0.6035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [13:27<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Training Loss: 0.6998, Validation Loss: 0.6938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [12:26<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training Loss: 0.6942, Validation Loss: 0.6687\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model_rnn_onthefly = RNNModelOnTheFly(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_rnn_onthefly.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model_rnn_onthefly, train_loader, val_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cde2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ada724b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModelOnTheFly(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTMModelOnTheFly, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Trainable embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embeds)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c28b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72176c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [13:40<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.6658, Validation Loss: 0.6705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [12:33<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Training Loss: 0.4360, Validation Loss: 0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [11:52<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Training Loss: 0.2572, Validation Loss: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [12:14<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Training Loss: 0.1829, Validation Loss: 0.3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [11:46<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Training Loss: 0.1247, Validation Loss: 0.3571\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model_lstm_onthefly = LSTMModelOnTheFly(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_lstm_onthefly.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(model_lstm_onthefly, train_loader, val_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0da65ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.squeeze().round().cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b4343f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GloVe RNN Model\n",
      "Accuracy: 0.4936\n",
      "Precision: 0.5294\n",
      "Recall: 0.0213\n",
      "F1 Score: 0.0409\n",
      "Evaluating GloVe LSTM Model\n",
      "Accuracy: 0.8780\n",
      "Precision: 0.8850\n",
      "Recall: 0.8731\n",
      "F1 Score: 0.8790\n",
      "Evaluating On-the-Fly RNN Model\n",
      "Accuracy: 0.6196\n",
      "Precision: 0.6039\n",
      "Recall: 0.7281\n",
      "F1 Score: 0.6602\n",
      "Evaluating On-the-Fly LSTM Model\n",
      "Accuracy: 0.8782\n",
      "Precision: 0.8737\n",
      "Recall: 0.8885\n",
      "F1 Score: 0.8810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8782,\n",
       " 'precision': 0.8736923672994963,\n",
       " 'recall': 0.88849487785658,\n",
       " 'f1': 0.8810314514553623}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the GloVe RNN model\n",
    "print(\"Evaluating GloVe RNN Model\")\n",
    "evaluate_model(model_rnn_glove, test_loader)\n",
    "\n",
    "# Evaluate the GloVe LSTM model\n",
    "print(\"Evaluating GloVe LSTM Model\")\n",
    "evaluate_model(model_lstm_glove, test_loader)\n",
    "\n",
    "# Evaluate the On-the-Fly RNN model\n",
    "print(\"Evaluating On-the-Fly RNN Model\")\n",
    "evaluate_model(model_rnn_onthefly, test_loader)\n",
    "\n",
    "# Evaluate the On-the-Fly LSTM model\n",
    "print(\"Evaluating On-the-Fly LSTM Model\")\n",
    "evaluate_model(model_lstm_onthefly, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008bfcf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf971d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
