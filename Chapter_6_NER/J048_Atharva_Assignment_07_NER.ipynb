{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy\nimport pandas\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-09-16T06:32:25.621502Z","iopub.execute_input":"2024-09-16T06:32:25.621829Z","iopub.status.idle":"2024-09-16T06:32:25.975451Z","shell.execute_reply.started":"2024-09-16T06:32:25.621795Z","shell.execute_reply":"2024-09-16T06:32:25.974672Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/train.json'))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:16:56.304432Z","iopub.execute_input":"2024-09-16T07:16:56.305288Z","iopub.status.idle":"2024-09-16T07:16:58.032553Z","shell.execute_reply.started":"2024-09-16T07:16:56.305227Z","shell.execute_reply":"2024-09-16T07:16:58.031754Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 1024\nOUTPUT_DIR = \"output\"","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:16:58.041841Z","iopub.execute_input":"2024-09-16T07:16:58.042132Z","iopub.status.idle":"2024-09-16T07:16:58.049163Z","shell.execute_reply.started":"2024-09-16T07:16:58.042100Z","shell.execute_reply":"2024-09-16T07:16:58.048300Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:16:17.203681Z","iopub.execute_input":"2024-09-16T07:16:17.204730Z","iopub.status.idle":"2024-09-16T07:16:35.027986Z","shell.execute_reply.started":"2024-09-16T07:16:17.204685Z","shell.execute_reply":"2024-09-16T07:16:35.026762Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:16:35.029940Z","iopub.execute_input":"2024-09-16T07:16:35.030243Z","iopub.status.idle":"2024-09-16T07:16:56.302233Z","shell.execute_reply.started":"2024-09-16T07:16:35.030208Z","shell.execute_reply":"2024-09-16T07:16:56.301099Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"p=[] # positive samples (contain relevant labels)\nn=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\nfor d in data:\n    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n    else: n.append(d)\nprint(\"original datapoints: \", len(data))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:17:36.274750Z","iopub.execute_input":"2024-09-16T07:17:36.275164Z","iopub.status.idle":"2024-09-16T07:17:37.930545Z","shell.execute_reply.started":"2024-09-16T07:17:36.275129Z","shell.execute_reply":"2024-09-16T07:17:37.929650Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"original datapoints:  6807\n","output_type":"stream"}]},{"cell_type":"code","source":"all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}\n\ntarget = [\n    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n]\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:18:00.943584Z","iopub.execute_input":"2024-09-16T07:18:00.944312Z","iopub.status.idle":"2024-09-16T07:18:01.022001Z","shell.execute_reply.started":"2024-09-16T07:18:00.944273Z","shell.execute_reply":"2024-09-16T07:18:01.021083Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize(example, tokenizer, label2id, max_length):\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        text.append(t)\n        labels.extend([l] * len(t))\n\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n\n    # actual tokenization\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n\n    labels = np.array(labels)\n\n    text = \"\".join(text)\n    token_labels = []\n    \n    for start_idx, end_idx in tokenized.offset_mapping:\n        # CLS token\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:51:08.274339Z","iopub.execute_input":"2024-09-16T07:51:08.275008Z","iopub.status.idle":"2024-09-16T07:51:08.286379Z","shell.execute_reply.started":"2024-09-16T07:51:08.274965Z","shell.execute_reply":"2024-09-16T07:51:08.285394Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [str(x[\"document\"]) for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    \"provided_labels\": [x[\"labels\"] for x in data],\n})\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, num_proc=3)\n# ds = ds.class_encode_column(\"group\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:51:20.064094Z","iopub.execute_input":"2024-09-16T07:51:20.064835Z","iopub.status.idle":"2024-09-16T07:51:56.761140Z","shell.execute_reply.started":"2024-09-16T07:51:20.064795Z","shell.execute_reply":"2024-09-16T07:51:56.760215Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e675a2de562499d92ef484d5fe6d047"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4a3004af9c4482d8acdf78267d1b5ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5843fcc1eed4320b5d42b9d9465e4f5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=3):   0%|          | 0/6807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8acb4f32c1bb4061b0663c918f5cbf25"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","source":"x = ds[0]\n\nfor t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n    if l != \"O\":\n        print((t,l))\n\nprint(\"*\"*100)\n\nfor t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n    if id2label[l] != \"O\":\n        print((t,id2label[l]))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:51:56.763215Z","iopub.execute_input":"2024-09-16T07:51:56.763575Z","iopub.status.idle":"2024-09-16T07:51:56.780022Z","shell.execute_reply.started":"2024-09-16T07:51:56.763523Z","shell.execute_reply":"2024-09-16T07:51:56.779035Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n('Nathalie', 'B-NAME_STUDENT')\n('Sylla', 'I-NAME_STUDENT')\n****************************************************************************************************\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('▁S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('▁S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n('N', 'B-NAME_STUDENT')\n('atha', 'B-NAME_STUDENT')\n('lie', 'B-NAME_STUDENT')\n('▁S', 'I-NAME_STUDENT')\n('ylla', 'I-NAME_STUDENT')\n","output_type":"stream"}]},{"cell_type":"code","source":"from seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:52:38.692358Z","iopub.execute_input":"2024-09-16T07:52:38.693228Z","iopub.status.idle":"2024-09-16T07:52:38.703577Z","shell.execute_reply.started":"2024-09-16T07:52:38.693186Z","shell.execute_reply":"2024-09-16T07:52:38.702751Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(p, all_labels):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:53:02.152266Z","iopub.execute_input":"2024-09-16T07:53:02.152662Z","iopub.status.idle":"2024-09-16T07:53:02.160501Z","shell.execute_reply.started":"2024-09-16T07:53:02.152623Z","shell.execute_reply":"2024-09-16T07:53:02.159486Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:53:09.482941Z","iopub.execute_input":"2024-09-16T07:53:09.483795Z","iopub.status.idle":"2024-09-16T07:53:13.213385Z","shell.execute_reply.started":"2024-09-16T07:53:09.483752Z","shell.execute_reply":"2024-09-16T07:53:13.212563Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44ab5810d6a44e8aa83890bba720c56"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    eval_strategy=\"no\",\n    do_eval=False,\n    save_total_limit=1,\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:53:58.734589Z","iopub.execute_input":"2024-09-16T07:53:58.734953Z","iopub.status.idle":"2024-09-16T07:53:58.763090Z","shell.execute_reply.started":"2024-09-16T07:53:58.734921Z","shell.execute_reply":"2024-09-16T07:53:58.762109Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model, \n    args=args, \n    train_dataset=ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:54:05.146364Z","iopub.execute_input":"2024-09-16T07:54:05.146764Z","iopub.status.idle":"2024-09-16T07:54:05.518849Z","shell.execute_reply.started":"2024-09-16T07:54:05.146725Z","shell.execute_reply":"2024-09-16T07:54:05.517996Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T07:54:18.239606Z","iopub.execute_input":"2024-09-16T07:54:18.240588Z","iopub.status.idle":"2024-09-16T09:01:15.499521Z","shell.execute_reply.started":"2024-09-16T07:54:18.240508Z","shell.execute_reply":"2024-09-16T09:01:15.498696Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2553' max='2553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2553/2553 1:06:53, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.878900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.177300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.445900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.368100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.012600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.008100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.011400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.012200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.010800</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.006900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.008300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.006000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.002900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.002700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.001200</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.001600</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.001400</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.001000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.000100</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.000700</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.000200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.000300</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.000500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2553, training_loss=0.07139591730089039, metrics={'train_runtime': 4016.3101, 'train_samples_per_second': 5.085, 'train_steps_per_second': 0.636, 'total_flos': 9485443697980512.0, 'train_loss': 0.07139591730089039, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"deberta3base_1024\")\ntokenizer.save_pretrained(\"deberta3base_1024\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:01:15.501174Z","iopub.execute_input":"2024-09-16T09:01:15.501481Z","iopub.status.idle":"2024-09-16T09:01:17.270225Z","shell.execute_reply.started":"2024-09-16T09:01:15.501449Z","shell.execute_reply":"2024-09-16T09:01:17.269164Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('deberta3base_1024/tokenizer_config.json',\n 'deberta3base_1024/special_tokens_map.json',\n 'deberta3base_1024/spm.model',\n 'deberta3base_1024/added_tokens.json',\n 'deberta3base_1024/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH = 2048\nmodel_path = '/kaggle/working/deberta3base_1024'","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:07:21.539911Z","iopub.execute_input":"2024-09-16T09:07:21.540916Z","iopub.status.idle":"2024-09-16T09:07:21.545007Z","shell.execute_reply.started":"2024-09-16T09:07:21.540872Z","shell.execute_reply":"2024-09-16T09:07:21.544012Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def tokenize(example, tokenizer):\n    text = []\n    token_map = []\n    \n    idx = 0\n    \n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        \n        text.append(t)\n        token_map.extend([idx]*len(t))\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH)\n    \n        \n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:07:24.251853Z","iopub.execute_input":"2024-09-16T09:07:24.252239Z","iopub.status.idle":"2024-09-16T09:07:24.258959Z","shell.execute_reply.started":"2024-09-16T09:07:24.252202Z","shell.execute_reply":"2024-09-16T09:07:24.258037Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:07:24.564309Z","iopub.execute_input":"2024-09-16T09:07:24.565060Z","iopub.status.idle":"2024-09-16T09:07:25.629360Z","shell.execute_reply.started":"2024-09-16T09:07:24.565020Z","shell.execute_reply":"2024-09-16T09:07:25.628417Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a70458b60d04d6cad122eac29c443ef"}},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(model_path)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=1, \n    report_to=\"none\",\n)\ntrainer = Trainer(\n    model=model, \n    args=args, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:07:37.251658Z","iopub.execute_input":"2024-09-16T09:07:37.252148Z","iopub.status.idle":"2024-09-16T09:07:37.598626Z","shell.execute_reply.started":"2024-09-16T09:07:37.252105Z","shell.execute_reply":"2024-09-16T09:07:37.597814Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"from pathlib import Path\n\npredictions = trainer.predict(ds).predictions\npred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n\nconfig = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]\npreds = predictions.argmax(-1)\npreds_without_O = pred_softmax[:,:,:12].argmax(-1)\nO_preds = pred_softmax[:,:,12]\n\nthreshold = 0.9\npreds_final = np.where(O_preds < threshold, preds_without_O , preds)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:08:08.708336Z","iopub.execute_input":"2024-09-16T09:08:08.708702Z","iopub.status.idle":"2024-09-16T09:08:09.443731Z","shell.execute_reply.started":"2024-09-16T09:08:08.708668Z","shell.execute_reply":"2024-09-16T09:08:09.442799Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"triplets = []\ndocument, token, label, token_str = [], [], [], []\nfor p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n\n    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n        label_pred = id2label[str(token_pred)]\n\n        if start_idx + end_idx == 0: continue\n\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # ignore \"\\n\\n\"\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n            if start_idx >= len(token_map): break\n\n            token_id = token_map[start_idx]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[token_id])\n\n                if triplet not in triplets:\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[token_id])\n                    triplets.append(triplet)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:08:51.479834Z","iopub.execute_input":"2024-09-16T09:08:51.480500Z","iopub.status.idle":"2024-09-16T09:08:51.572869Z","shell.execute_reply.started":"2024-09-16T09:08:51.480460Z","shell.execute_reply":"2024-09-16T09:08:51.572143Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df = pandas.DataFrame({\n    \"document\": document,\n    \"token\": token,\n    \"label\": label,\n    \"token_str\": token_str\n})\ndf[\"row_id\"] = list(range(len(df)))\ndisplay(df.head(3))","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:09:21.277765Z","iopub.execute_input":"2024-09-16T09:09:21.278671Z","iopub.status.idle":"2024-09-16T09:09:21.290454Z","shell.execute_reply.started":"2024-09-16T09:09:21.278627Z","shell.execute_reply":"2024-09-16T09:09:21.289482Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"   document  token           label token_str  row_id\n0        10    464  B-NAME_STUDENT     Diego       0\n1        20      5  B-NAME_STUDENT     Sindy       1\n2        20      8  I-NAME_STUDENT     Gitam       2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>token</th>\n      <th>label</th>\n      <th>token_str</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10</td>\n      <td>464</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Diego</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>5</td>\n      <td>B-NAME_STUDENT</td>\n      <td>Sindy</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20</td>\n      <td>8</td>\n      <td>I-NAME_STUDENT</td>\n      <td>Gitam</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}